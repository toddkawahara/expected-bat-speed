# -*- coding: utf-8 -*-
"""Predicting Bat Speed.ipynb

Automatically generated by Colaboratory.

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.width', 1000)
pd.set_option('display.max_rows', 200)
pd.set_option('display.max_columns', 200)
pd.set_option('display.max_colwidth', 50)

## Loading in the data, dropping null values
df = pd.read_csv('/content/drive/MyDrive/Datasets/hitting biomechanics/poi_metrics.csv')
df = df.dropna()

"""## EDA"""

df.shape

# Bat speed distribution
sns.histplot(df['bat_speed_mph_max_x'], kde=True)
plt.title('Bat Speed Distribution')
plt.xlabel('Bat Speed MPH')
plt.show()

## Linear relationship plot between bat speed and exit velo
regplot = sns.regplot(data=df, x='bat_speed_mph_max_x', y='exit_velo_mph_x')

slope, intercept = np.polyfit(df['bat_speed_mph_max_x'], df['exit_velo_mph_x'], 1)

## Generating line equation
equation = f'y = {slope: .2f}x + {intercept: .2f}'
regplot.annotate(equation, (0.6, 0.2), xycoords = 'axes fraction', fontsize=12, color='black')
plt.xlabel('Bat Speed MPH')
plt.ylabel('Exit Velo MPH')
plt.title('Bat Speed vs Exit Velo')

plt.show()

## Examining the feautres that have the highest correlations with bat speed
correlations = df.corr(numeric_only=True)['bat_speed_mph_max_x']
correlations = abs(correlations)
correlations.sort_values(ascending=False).head(20)

## Dropping irrelevant columns and features that have to do with bat speed, the thing we are trying to predict
cols = df.columns
cols = cols.drop(['session', 'exit_velo_mph_x', 'blast_bat_speed_mph_x', 'bat_speed_mph_contact_x', 'sweet_spot_velo_mph_contact_x', 'sweet_spot_velo_mph_contact_y',
                  'sweet_spot_velo_mph_contact_z', 'attack_angle_contact_x', 'bat_speed_xy_max_x', 'bat_max_x', 'bat_min_x',
                  'hand_speed_mag_max_x', 'hand_speed_blast_bat_mph_max_x', 'hand_speed_mag_swing_max_velo_x', 'hand_speed_mag_stride_max_velo_x', 'hand_speed_mag_seq_max_x',
                  'hand_speed_mag_maxhss_x', 'hand_speed_mag_fp_x', 'hand_speed_mag_fm_x'])
df = df[cols]

# Checking highest correlations in general
high_corr = df.corr(numeric_only=True).unstack().sort_values(ascending=False)
high_corr.loc[:, high_corr != 1].head(20)

## Removing features that have correlations greater than 0.75 with another so as to reduce redundant information, of these features keeping the one that has the highest correlation with bat speed

# Calculate the correlation matrix
correlation_matrix = df.corr(numeric_only=True)

# Set the threshold for correlation
threshold = 0.75

# Find correlations greater than the threshold
high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

# Initialize a set to store columns to remove
columns_to_remove = set()

# Iterate through columns with high correlations
for col in high_correlations.columns:
    correlated_columns = high_correlations.index[high_correlations[col]]
    if correlated_columns.any():
        max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
        columns_to_remove.update(correlated_columns.difference([max_corr_column]))

# Keep the 'bat_speed_mph_max_x' column
columns_to_remove.discard('bat_speed_mph_max_x')

# Keep the first column from each correlated group
columns_to_keep = set(df.columns) - columns_to_remove

# Create a new DataFrame with only the columns to keep
columns_to_keep = list(columns_to_keep)
df_1 = df[columns_to_keep]

"""## Initializing and testing models"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.metrics import mean_squared_error

## Initializing and testing various models
feats = df_1.columns.drop(['bat_speed_mph_max_x', 'session_swing'])
rf_name = [RandomForestRegressor(), 'Random Forest']
xgb_name = [XGBRegressor(), 'XGBoost']

for model, name in (rf_name, xgb_name):
    model = model

    # Generating predictions
    preds = cross_val_predict(model, df[feats], df['bat_speed_mph_max_x'], cv=5)

    # Generating RMSE
    print(name)
    print('RMSE: ' + str(mean_squared_error(preds, df['bat_speed_mph_max_x'])**0.5))

    # Generating slope of line of best fit
    slope, _ = np.polyfit(df['bat_speed_mph_max_x'], preds, deg=1)
    print("Slope of the line of best fit:", slope)

    # Plotting predictions against actual bat speed
    sns.regplot(x=df['bat_speed_mph_max_x'], y=preds)
    plt.ylabel('predicted_bat_speed')
    plt.title(name + ' Predictions')
    plt.ylabel('Predicted Bat Speed')
    plt.xlabel('Actual Bat Speed')
    plt.xlim([50, 85])
    plt.ylim([50, 85])
    plt.show()

    print('*'*100)

## Testing how regressions do with scaled data
scaler = StandardScaler()
X = df[feats]
X_scaled = scaler.fit_transform(X)

models = [LinearRegression(), Ridge(), Lasso()]

for x in models:
    model = x
    print(str(model) + ' RMSE: ' + str(cross_val_score(model, X_scaled, df['bat_speed_mph_max_x'], cv=5, scoring='neg_root_mean_squared_error').mean()))

    # Generating predictions
    preds = cross_val_predict(x, X_scaled, df['bat_speed_mph_max_x'], cv=5)

    # Generating slope of line of best fit
    slope, _ = np.polyfit(df['bat_speed_mph_max_x'], preds, deg=1)
    print("Slope of the line of best fit:", slope)

    # Plotting predictions against actual bat speed
    sns.regplot(x=df['bat_speed_mph_max_x'], y=preds)
    plt.title(f'{x} Predictions')
    plt.ylabel('Predicted Bat Speed')
    plt.xlabel('Actual Bat Speed')
    plt.xlim([50, 85])
    plt.ylim([50, 85])
    plt.show()
    print('*'*100)

"""## Examining how removing features by correlations affects performance"""

## Examining how the RMSE and line of best fit slope changes as the correlation threshold of what features to remove changes for Linear Regression
lr_thresholds = []
lr_rmse_scores = []
lr_slopes = []

for x in np.arange(0.25, 0.901, 0.0025):
    print('Iteration: ' + str(x))
    # Calculate the correlation matrix
    correlation_matrix = df.corr(numeric_only=True)

    # Set the threshold for correlation
    threshold = x
    lr_thresholds.append(x)

    # Find correlations greater than the threshold
    high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

    # Initialize a set to store columns to remove
    columns_to_remove = set()

    # Iterate through columns with high correlations
    for col in high_correlations.columns:
        correlated_columns = high_correlations.index[high_correlations[col]]
        if correlated_columns.any():
            max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
            columns_to_remove.update(correlated_columns.difference([max_corr_column]))

    # Keep the 'bat_speed_mph_max_x' column
    columns_to_remove.discard('bat_speed_mph_max_x')

    # Keep the first column from each correlated group
    columns_to_keep = set(df.columns) - columns_to_remove

    # Create a new DataFrame with only the columns to keep
    columns_to_keep = list(columns_to_keep)
    df_1 = df[columns_to_keep]
    feats = df_1.columns.drop(['bat_speed_mph_max_x', 'session_swing'])

    # Initializing model
    lr = LinearRegression()

    # Scaling the data
    scaler = StandardScaler()
    X = df[feats]
    X_scaled = scaler.fit_transform(X)

    # Generating predictions
    preds = cross_val_predict(lr, X_scaled, df['bat_speed_mph_max_x'], cv=5)

    # Generating and appending RMSE
    rmse = (mean_squared_error(preds, df['bat_speed_mph_max_x'])**0.5)
    lr_rmse_scores.append(rmse)

    # Generating and appending slopes
    slope, _ = np.polyfit(df['bat_speed_mph_max_x'], preds, deg=1)
    lr_slopes.append(slope)

## Plotting the RMSE and slope at the differing corrleations for Linear Regression
lr_abs_rmse = [abs(ele) for ele in lr_rmse_scores]

# Create the figure and left y-axis
fig, ax1 = plt.subplots(figsize=(7, 5))
ax1.scatter(lr_thresholds, lr_abs_rmse, color='blue', marker='o', label='RMSE')
ax1.set_xlabel('Correlation')
ax1.set_ylabel('RMSE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_ylim([0, 7])

# Create the right y-axis
ax2 = ax1.twinx()
ax2.scatter(lr_thresholds, lr_slopes, color='red', marker='x', label='Slope')
ax2.set_ylabel('Slope', color='red')
ax2.tick_params(axis='y', labelcolor='red')
ax2.set_ylim([-0.1, 0.3])

# Add vertical grid lines
for x_value in range(0, len(lr_thresholds), 5):
    ax1.axvline(x=lr_thresholds[x_value], color='gray', linestyle='--', linewidth=0.5)

# Add a legend
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
lines = lines1 + lines2
labels = labels1 + labels2
ax1.legend(lines, labels, loc='upper left')

# Show the plot
plt.title('Linear Regression RMSE and Line of Best Fit Slope as \n Threshold of Variable Correlations to Remove Changes')
# plt.grid(True, axis='both')
plt.show()

7/8

## Examining how the RMSE and line of best fit slope changes as the correlation threshold of what features to remove changes for Ridge Regression
ridge_thresholds = []
ridge_rmse_scores = []
ridge_slopes = []

for x in np.arange(0.25, 0.901, 0.0025):
    print('Iteration: ' + str(x))
    # Calculate the correlation matrix
    correlation_matrix = df.corr(numeric_only=True)

    # Set the threshold for correlation
    threshold = x
    ridge_thresholds.append(x)

    # Find correlations greater than the threshold
    high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

    # Initialize a set to store columns to remove
    columns_to_remove = set()

    # Iterate through columns with high correlations
    for col in high_correlations.columns:
        correlated_columns = high_correlations.index[high_correlations[col]]
        if correlated_columns.any():
            max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
            columns_to_remove.update(correlated_columns.difference([max_corr_column]))

    # Keep the 'bat_speed_mph_max_x' column
    columns_to_remove.discard('bat_speed_mph_max_x')

    # Keep the first column from each correlated group
    columns_to_keep = set(df.columns) - columns_to_remove

    # Create a new DataFrame with only the columns to keep
    columns_to_keep = list(columns_to_keep)
    df_1 = df[columns_to_keep]
    feats = df_1.columns.drop(['bat_speed_mph_max_x', 'session_swing'])

    # Initializing model
    ridge = Ridge()

    # Scaling the data
    scaler = StandardScaler()
    X = df[feats]
    X_scaled = scaler.fit_transform(X)

    # Generating predictions
    preds = cross_val_predict(ridge, X_scaled, df['bat_speed_mph_max_x'], cv=5)

    # Generating and appending RMSE
    rmse = (mean_squared_error(preds, df['bat_speed_mph_max_x'])**0.5)
    ridge_rmse_scores.append(rmse)

    # Generating and appending slopes
    slope, _ = np.polyfit(df['bat_speed_mph_max_x'], preds, deg=1)
    ridge_slopes.append(slope)

## Plotting the RMSE and slope at the differing corrleations for Ridge Regression
ridge_abs_rmse = [abs(ele) for ele in ridge_rmse_scores]

# Create the figure and left y-axis
fig, ax1 = plt.subplots(figsize=(7, 5))
ax1.scatter(ridge_thresholds, ridge_abs_rmse, color='blue', marker='o', label='RMSE')
ax1.set_xlabel('Correlation')
ax1.set_ylabel('RMSE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_ylim([0, 7])

# Create the right y-axis
ax2 = ax1.twinx()
ax2.scatter(ridge_thresholds, ridge_slopes, color='red', marker='x', label='Slope')
ax2.set_ylabel('Slope', color='red')
ax2.tick_params(axis='y', labelcolor='red')
ax2.set_ylim([-0.1, 0.3])

# Add vertical grid lines
for x_value in range(0, len(ridge_thresholds), 5):
    ax1.axvline(x=ridge_thresholds[x_value], color='gray', linestyle='--', linewidth=0.5)

# Add a legend
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
lines = lines1 + lines2
labels = labels1 + labels2
ax1.legend(lines, labels, loc='upper left')

# Show the plot
plt.title('Ridge Regression RMSE and Line of Best Fit Slope as \n Threshold of Variable Correlations to Remove Changes')
plt.show()

((6/8) + (7/8))/2

## Examining how the RMSE and line of best fit slope changes as the correlation threshold of what features to remove changes for XGBoost
xgb_thresholds = []
xgb_rmse_scores = []
xgb_slopes = []

for x in np.arange(0.25, 0.901, 0.005):
    print('Iteration: ' + str(x))
    # Calculate the correlation matrix
    correlation_matrix = df.corr(numeric_only=True)

    # Set the threshold for correlation
    threshold = x
    xgb_thresholds.append(x)

    # Find correlations greater than the threshold
    high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

    # Initialize a set to store columns to remove
    columns_to_remove = set()

    # Iterate through columns with high correlations
    for col in high_correlations.columns:
        correlated_columns = high_correlations.index[high_correlations[col]]
        if correlated_columns.any():
            max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
            columns_to_remove.update(correlated_columns.difference([max_corr_column]))

    # Keep the 'bat_speed_mph_max_x' column
    columns_to_remove.discard('bat_speed_mph_max_x')

    # Keep the first column from each correlated group
    columns_to_keep = set(df.columns) - columns_to_remove

    # Create a new DataFrame with only the columns to keep
    columns_to_keep = list(columns_to_keep)
    df_1 = df[columns_to_keep]
    feats = df_1.columns.drop(['bat_speed_mph_max_x', 'session_swing'])

    # Initializing model
    xgb = XGBRegressor()

    # Generating predictions
    preds = cross_val_predict(xgb, df[feats], df['bat_speed_mph_max_x'], cv=5)

    # Generating and appending RMSE
    rmse = (mean_squared_error(preds, df['bat_speed_mph_max_x'])**0.5)
    xgb_rmse_scores.append(rmse)

    # Generating and appending slopes
    slope, _ = np.polyfit(df['bat_speed_mph_max_x'], preds, deg=1)
    xgb_slopes.append(slope)

## Plotting the RMSE and slope at the differing corrleations for XGBoost Regression
xgb_abs_rmse = [abs(ele) for ele in xgb_rmse_scores]

# Create the figure and left y-axis
fig, ax1 = plt.subplots(figsize=(7, 5))
ax1.scatter(xgb_thresholds, xgb_abs_rmse, color='blue', marker='o', label='RMSE')
ax1.set_xlabel('Correlation')
ax1.set_ylabel('RMSE', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_ylim([0, 7])

# Create the right y-axis
ax2 = ax1.twinx()
ax2.scatter(xgb_thresholds, xgb_slopes, color='red', marker='x', label='Slope')
ax2.set_ylabel('Slope', color='red')
ax2.tick_params(axis='y', labelcolor='red')
ax2.set_ylim([-0.1, 0.3])

# Add vertical grid lines
for x_value in range(0, len(xgb_thresholds), 5):
    ax1.axvline(x=xgb_thresholds[x_value], color='gray', linestyle='--', linewidth=0.5)

# Add a legend
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
lines = lines1 + lines2
labels = labels1 + labels2
ax1.legend(lines, labels, loc='upper left')

# Show the plot
plt.title('XGBoost Regression RMSE and Line of Best Fit Slope as \n Threshold of Variable Correlations to Remove Changes')
plt.show()

## Creating dataframes with the features that led to the best performances in the Ridge Regression and XGBoost Regression

# Calculate the correlation matrix
correlation_matrix = df.corr(numeric_only=True)

## Ridge dataframe
# Set the threshold for correlation
threshold = 0.48125

# Find correlations greater than the threshold
high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

# Initialize a set to store columns to remove
columns_to_remove = set()

# Iterate through columns with high correlations
for col in high_correlations.columns:
    correlated_columns = high_correlations.index[high_correlations[col]]
    if correlated_columns.any():
        max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
        columns_to_remove.update(correlated_columns.difference([max_corr_column]))

# Keep the 'bat_speed_mph_max_x' column
columns_to_remove.discard('bat_speed_mph_max_x')

# Keep the first column from each correlated group
columns_to_keep = set(df.columns) - columns_to_remove

# Create a new DataFrame with only the columns to keep
columns_to_keep = list(columns_to_keep)
ridge_df = df[columns_to_keep]
ridge_feats = ridge_df.columns.drop(['bat_speed_mph_max_x', 'session_swing'])

## XGBoost dataframe
# Set the threshold for correlation
threshold = 0.45

# Find correlations greater than the threshold
high_correlations = (correlation_matrix.abs() > threshold) & (correlation_matrix.index != 'bat_speed_mph_max_x')

# Initialize a set to store columns to remove
columns_to_remove = set()

# Iterate through columns with high correlations
for col in high_correlations.columns:
    correlated_columns = high_correlations.index[high_correlations[col]]
    if correlated_columns.any():
        max_corr_column = correlation_matrix.loc[correlated_columns, 'bat_speed_mph_max_x'].abs().idxmax()
        columns_to_remove.update(correlated_columns.difference([max_corr_column]))

# Keep the 'bat_speed_mph_max_x' column
columns_to_remove.discard('bat_speed_mph_max_x')

# Keep the first column from each correlated group
columns_to_keep = set(df.columns) - columns_to_remove

# Create a new DataFrame with only the columns to keep
columns_to_keep = list(columns_to_keep)
xgb_df = df[columns_to_keep]
xgb_feats = xgb_df.columns.drop(['bat_speed_mph_max_x', 'session_swing'])

ridge_feats

xgb_feats

"""## Learning curves"""

from sklearn.model_selection import learning_curve

for model, name, feats in ([Ridge(), 'Ridge Regression', ridge_feats], [XGBRegressor(), 'XGBoost', xgb_feats]):
    model = model

    # Define the range of training set sizes you want to use
    train_sizes = np.linspace(0.1, 1.0, 10)

    # Generate the learning curve
    train_sizes, train_scores, test_scores = learning_curve(
        model, df[feats], df['bat_speed_mph_max_x'], train_sizes=train_sizes, cv=5, scoring='neg_root_mean_squared_error', shuffle=True
    )

    # Calculate mean and standard deviation of training and validation scores
    train_scores_mean = -np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Create a learning curve plot
    plt.figure(figsize=(10, 6))
    plt.title(f"{name} Learning Curve")
    plt.xlabel("Number of Training Examples")
    plt.ylabel("Root Mean Squared Error")
    plt.grid()

    # Plot the learning curve
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="blue")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="orange")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="blue", label="Training Score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="orange", label="Cross-Validation Score")

    # Customize the plot as needed
    plt.legend(loc="best")
    plt.show()

model = XGBRegressor(max_depth=4,
    min_child_weight=1,
    gamma=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    colsample_bylevel=0.8,
    reg_alpha=0.01,  # L1 regularization
    reg_lambda=1.0,  # L2 regularization
    learning_rate=0.1  # Learning rate (eta)
    )

# Define the range of training set sizes you want to use
train_sizes = np.linspace(0.1, 1.0, 10)

# Generate the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    model, df[xgb_feats], df['bat_speed_mph_max_x'], train_sizes=train_sizes, cv=5, scoring='neg_root_mean_squared_error'
)

# Calculate mean and standard deviation of training and validation scores
train_scores_mean = -np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = -np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Create a learning curve plot
plt.figure(figsize=(10, 6))
plt.title("Reduced Complexity XGBoost Learning Curve")
plt.xlabel("Number of Training Examples")
plt.ylabel("Negative Root Mean Squared Error")
plt.grid()

# Plot the learning curve
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="blue")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="orange")
plt.plot(train_sizes, train_scores_mean, 'o-', color="blue", label="Training Score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="orange", label="Cross-Validation Score")

# Customize the plot as needed
plt.legend(loc="best")
plt.show()

"""## Hyperparameter tuning"""

!pip install optuna

import optuna

## Scaling the ridge dataframe
scaler = StandardScaler()
X = ridge_df[ridge_feats]
X_scaled = scaler.fit_transform(X)

# Tuning Ridge Regression alpha using optuna
def objective(trial):
    # Define the hyperparameter to tune
    alpha = trial.suggest_loguniform('alpha', 500, 2500)

    # Create and evaluate the Ridge Regression model
    model = Ridge(alpha=alpha)
    score = cross_val_score(model, X_scaled, df['bat_speed_mph_max_x'], cv=5, scoring='neg_root_mean_squared_error').mean()

    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
print("Best Hyperparameters:", best_params)

"""Best is trial 56 with value: -4.731295910332894.
Best Hyperparameters: {'alpha': 951.8667895701567}
"""

# Tuning XGBoost model using optuna
def objective(trial):
    # Define the hyperparameter to tune
    params = {
    'objective':'reg:squarederror',
    'eval_metric': 'rmse',
    'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),
    'lambda': trial.suggest_loguniform('lambda', 1e-5,1),
    'alpha': trial.suggest_loguniform('alpha', 1e-5, 1),
    'max_depth': trial.suggest_int('max_depth', 3, 12),
    'eta': trial.suggest_loguniform('eta', 0.001, 1),
    'gamma': trial.suggest_loguniform('gamma', 1e-5, 1),
    'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-5, 1),
    'subsample': trial.suggest_uniform('subsample', 0.1, 1),
    'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1)}

    # Create and evaluate the XGBoost Regression model
    model = XGBRegressor(**params, random_state=42)
    score = cross_val_score(model, xgb_df[xgb_feats], df['bat_speed_mph_max_x'], cv=5, scoring='neg_root_mean_squared_error').mean()

    return score


study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
print("Best Hyperparameters:", best_params)

"""Best is trial 99 with value: -4.697522493603471.
Best Hyperparameters: {'booster': 'gbtree', 'lambda': 0.004286761991197219, 'alpha': 2.461211565378674e-05, 'max_depth': 11, 'eta': 0.08143697508843639, 'gamma': 0.12535220227537958, 'min_child_weight': 0.00020657997760769987, 'subsample': 0.42497823115072997, 'colsample_bytree': 0.8936751195548178}

## Predictions
"""

from sklearn.model_selection import KFold
from sklearn.pipeline import make_pipeline

## Generating out of sample predictions for Ridge and XGBoost regressions
kf = KFold(5, shuffle=True, random_state=1)
ridge_oos_preds = pd.Series(dtype=float)
xgb_oos_preds = pd.Series(dtype=float)
df = df.reset_index(drop=True)

for train_index, test_index in kf.split(df):
    train = df.loc[train_index]
    test = df.loc[test_index]

    # Scaling data for Ridge Regression
    scaler = StandardScaler()
    X_train = train[ridge_feats]
    X_test = test[ridge_feats]
    X_train_scaled = scaler.fit_transform(X_train)
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
    X_test_scaled = scaler.transform(X_test)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

    # Generating OOS Ridge Regression predictions
    ridge = Ridge(alpha= 951.8667895701567, random_state=42)
    ridge.fit(X_train_scaled[ridge_feats], train['bat_speed_mph_max_x'])
    test['ridge_predicted_bat_speed'] = ridge.predict(X_test_scaled[ridge_feats])
    ridge_oos_preds = pd.concat([ridge_oos_preds, test['ridge_predicted_bat_speed']])

    # Generating OOS XGBoost predictions
    xgb = XGBRegressor(booster='gbtree', reg_lambda=0.004286761991197219, alpha =2.461211565378674e-05, max_depth = 11, eta = 0.08143697508843639, gamma = 0.12535220227537958,
                       min_child_weight = 0.00020657997760769987, subsample = 0.42497823115072997, colsample_bytree = 0.8936751195548178,
                       random_state=42)
    xgb.fit(train[xgb_feats], train['bat_speed_mph_max_x'])
    test['xgb_predicted_bat_speed'] = xgb.predict(test[xgb_feats])
    xgb_oos_preds = pd.concat([xgb_oos_preds, test['xgb_predicted_bat_speed']])

df['ridge_predicted_bat_speed'] = ridge_oos_preds
df['xgb_predicted_bat_speed'] = xgb_oos_preds

# Generating Ridge Regression predictions using cross val predict
scaler = StandardScaler()
ridge = Ridge(alpha= 951.8667895701567, random_state=42)
ridge_pipeline = make_pipeline(StandardScaler(), ridge)
ridge_preds = cross_val_predict(ridge_pipeline, df[ridge_feats], df['bat_speed_mph_max_x'], cv=5)
ridge_preds = pd.Series(ridge_preds, index = df.index)
df['cvp_ridge_predicted_bat_speed'] = ridge_preds

# Generating XGBoost predictions using cross val predict
xgb = XGBRegressor(booster='gbtree', reg_lambda=0.004286761991197219, alpha =2.461211565378674e-05, max_depth = 11, eta = 0.08143697508843639, gamma = 0.12535220227537958,
                    min_child_weight = 0.00020657997760769987, subsample = 0.42497823115072997, colsample_bytree = 0.8936751195548178,
                   random_state=42)
xgb_preds = cross_val_predict(xgb, df[xgb_feats], df['bat_speed_mph_max_x'], cv=5)
xgb_preds = pd.Series(xgb_preds, index = df.index)
df['cvp_xgb_predicted_bat_speed'] = xgb_preds

"""## Model Evaluation"""

from scipy.stats import pearsonr

## Evaluating the ridge regression predictions generated by manually going through 5 KFolds

# R-Squared and RMSE
r2 = pearsonr(df['bat_speed_mph_max_x'], df['ridge_predicted_bat_speed'])[0]**2
rmse = mean_squared_error(df['bat_speed_mph_max_x'], df['ridge_predicted_bat_speed'])**0.5

print('Ridge Regression OOS R-Squared: ' + str(round(r2, 3)))
print('Ridge Regression OOS R-Squared: ' + str(round(rmse, 2)))

# Generating slope
slope, _ = np.polyfit(df['bat_speed_mph_max_x'], df['ridge_predicted_bat_speed'], deg=1)
print("Slope of the regression line:", slope)

# Plotting predictions against actual
sns.regplot(x=df['bat_speed_mph_max_x'], y=df['ridge_predicted_bat_speed'])
plt.xlim([50,85])
plt.ylim([50, 85])

plt.plot([50,85], [50,85], color='blue')
plt.title(f'Ridge Regression OOS Predicted vs Actual \n RMSE: {round(rmse, 2)}')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

## Evaluating the XGBoost regression predictions generated by manually going through 5 KFolds

# R-Squared and RMSE
r2 = pearsonr(df['bat_speed_mph_max_x'], df['xgb_predicted_bat_speed'])[0]**2
rmse = mean_squared_error(df['bat_speed_mph_max_x'], df['xgb_predicted_bat_speed'])**0.5

print('XGBoost Regression OOS R-Squared: ' + str(round(r2, 2)))
print('XGBoost Regression OOS RMSE: ' + str(round(rmse, 2)))

# Generating slope
slope, _ = np.polyfit(df['bat_speed_mph_max_x'], df['xgb_predicted_bat_speed'], deg=1)
print("Slope of the regression line:", slope)

# Plotting predictions against actual
sns.regplot(x=df['bat_speed_mph_max_x'], y=df['xgb_predicted_bat_speed'])
plt.xlim([50,85])
plt.ylim([50, 85])

plt.plot([50,85], [50,85], color='blue')
plt.title(f'XGBoost OOS Predicted vs Actual \n RMSE: {round(rmse,2)}')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

## Evaluating the Ridge Regression predictions generated by cross val predict

# R-Squared and RMSE
r2 = pearsonr(df['bat_speed_mph_max_x'], df['cvp_ridge_predicted_bat_speed'])[0]**2
rmse = mean_squared_error(df['bat_speed_mph_max_x'], df['cvp_ridge_predicted_bat_speed'])**0.5

print('Ridge Regression cross val predict R-Squared: ' + str(round(r2, 2)))
print('Ridge Regression cross val predict RMSE: ' + str(round(rmse, 2)))

# Generating slope
slope, _ = np.polyfit(df['bat_speed_mph_max_x'], df['cvp_ridge_predicted_bat_speed'], deg=1)
print("Slope of the regression line:", slope)

# Plotting predictions against actual
sns.regplot(x=df['bat_speed_mph_max_x'], y=df['cvp_ridge_predicted_bat_speed'])
plt.xlim([50,85])
plt.ylim([50, 85])

plt.plot([50,85], [50,85], color='blue')
plt.title(f'Ridge Regression Cross Val Predict Predicted vs Actual \n RMSE: {round(rmse,2)}')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

## Evaluating the XGBoost regression predictions generated by cross val predict

# R-Squared and RMSE
r2 = pearsonr(df['bat_speed_mph_max_x'], df['cvp_xgb_predicted_bat_speed'])[0]**2
rmse = mean_squared_error(df['bat_speed_mph_max_x'], df['cvp_xgb_predicted_bat_speed'])**0.5

print('XGBoost cross val predict R-Squared: ' + str(round(r2, 2)))
print('XGBoost cross val predict RMSE: ' + str(round(rmse, 2)))

# Generating slope
slope, _ = np.polyfit(df['bat_speed_mph_max_x'], df['cvp_xgb_predicted_bat_speed'], deg=1)
print("Slope of the regression line:", slope)

# Plotting predictions against actual
sns.regplot(x=df['bat_speed_mph_max_x'], y=df['cvp_xgb_predicted_bat_speed'])
plt.xlim([50,85])
plt.ylim([50, 85])

plt.plot([50,85], [50,85], color='blue')
plt.title(f'XGBoost Cross Val Predict Predicted vs Actual \n RMSE: {round(rmse,2)}')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()

## Tuned XGBoost Learning Curve

# Define the range of training set sizes you want to use
train_sizes = np.linspace(0.1, 1.0, 10)

# Generate the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    xgb, df[xgb_feats], df['bat_speed_mph_max_x'], train_sizes=train_sizes, cv=5, scoring='neg_root_mean_squared_error'
)

# Calculate mean and standard deviation of training and validation scores
train_scores_mean = -np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = -np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Create a learning curve plot
plt.figure(figsize=(10, 6))
plt.title("Tuned XGBoost Learning Curve")
plt.xlabel("Number of Training Examples")
plt.ylabel("Negative Root Mean Squared Error")
plt.grid()

# Plot the learning curve
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="blue")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="orange")
plt.plot(train_sizes, train_scores_mean, 'o-', color="blue", label="Training Score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="orange", label="Cross-Validation Score")

# Customize the plot as needed
plt.legend(loc="best")
plt.show()

"""## Feature Importance"""

# Fitting model to entire dataset
xgb.fit(df[xgb_feats], df['bat_speed_mph_max_x'])

# Getting feature importances
feature_importance = xgb.feature_importances_

# Feature Importance df
feature_importance_df = pd.DataFrame({'Feature': xgb_feats, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)

# Plotting the importances
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('XGBoost Feature Importance')
plt.show()

from scipy.stats import ttest_ind

def feature_plot(feature):
    # Getting pearson r
    r = pearsonr(df[feature], df['bat_speed_mph_max_x'])

    # Plot title
    title_str = f"""
    {feature}
    Pearson: {round(r[0], 3)} (p={round(r[1], 3)})
    """

    # Plotting the data
    regplot = sns.regplot(data=df, x=feature, y='bat_speed_mph_max_x')


    ## Generating line equation
    slope, intercept = np.polyfit(df[feature], df['bat_speed_mph_max_x'], 1)
    equation = f'y = {slope: .4f}x + {intercept: .2f}'
    regplot.annotate(equation, (0.1, 0.1), xycoords = 'axes fraction', fontsize=12, color='black')

    plt.title(title_str)
    plt.ylabel('Bat Speed MPH')
    plt.show

feature_plot('pelvis_angular_velocity_maxhss_x')

feature_plot('torso_stride_max_z')

feature_plot('upper_arm_speed_mag_max_x')

